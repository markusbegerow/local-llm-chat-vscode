{
  "name": "local-llm-chat-vscode",
  "displayName": "Local LLM for VS Code",
  "description": "Secure chat with local LLMs and create files in your workspace with AI assistance.",
  "version": "1.0.4",
  "publisher": "MarkusBegerow",
  "icon": "resources/robot_icon.png",
  "author": {
	"name": "Markus Begerow"
  },
  "contributors": [
	"Markus Begerow"
  ],
  "engines": {
    "vscode": "^1.85.0"
  },
  "categories": [
	"Extension Packs",
    "Other",
    "AI",
    "Chat"
  ],
  "repository": {
    "type": "git",
    "url": "https://github.com/markusbegerow/local-llm-chat-vscode"
  },
  "homepage": "https://github.com/markusbegerow/local-llm-chat-vscode#readme",
  "bugs": { "url": "https://github.com/markusbegerow/local-llm-chat-vscode/issues" },
  "sponsor": {
	"url": "https://github.com/sponsors/markusbegerow"
  },
  "license": "MIT",
  "keywords": [
    "llm",
    "ollama",
    "ai",
    "chat",
    "code-assistant"
  ],
  "activationEvents": [
    "onCommand:localLLM.openChat",
    "onCommand:localLLM.newFileFromSelection",
    "onCommand:localLLM.clearConversation",
    "onCommand:localLLM.sendFileToChat",
    "onCommand:localLLM.sendActiveFileToChat",
    "onCommand:localLLM.listWorkspaceFiles",
    "onCommand:localLLM.getWorkspaceInfo",
    "onCommand:localLLM.searchFiles"
  ],
  "main": "./out/extension.js",
  "contributes": {
    "commands": [
      {
        "command": "localLLM.openChat",
        "title": "Local LLM: Open Chat",
        "icon": "$(comment-discussion)"
      },
      {
        "command": "localLLM.newFileFromSelection",
        "title": "Local LLM: Create File From Selection"
      },
      {
        "command": "localLLM.clearConversation",
        "title": "Local LLM: Clear Conversation History"
      },
      {
        "command": "localLLM.sendFileToChat",
        "title": "Local LLM: Send File to Chat"
      },
      {
        "command": "localLLM.sendActiveFileToChat",
        "title": "Local LLM: Send Active File to Chat"
      },
      {
        "command": "localLLM.listWorkspaceFiles",
        "title": "Local LLM: List Workspace Files"
      },
      {
        "command": "localLLM.getWorkspaceInfo",
        "title": "Local LLM: Get Workspace Info"
      },
      {
        "command": "localLLM.searchFiles",
        "title": "Local LLM: Search Files"
      }
    ],
    "configuration": {
      "title": "Local LLM Chat",
      "properties": {
        "localLLM.apiUrl": {
          "type": "string",
          "default": "http://localhost:11434/v1/chat/completions",
          "markdownDescription": "Full API endpoint URL.\n\n**Examples:**\n- OpenAI: `https://api.openai.com/v1/chat/completions`\n- Ollama: `http://localhost:11434/v1/chat/completions`\n- Custom: `http://localhost:1234/v1/chat/completions`",
          "order": 1
        },
        "localLLM.token": {
          "type": "string",
          "default": "ollama",
          "markdownDescription": "API authentication token.\n\n**Examples:**\n- OpenAI: `sk-your-openai-api-key-here`\n- Ollama: `ollama` (or any dummy value)\n- Custom: `your-token-or-dummy-value`",
          "order": 2
        },
        "localLLM.model": {
          "type": "string",
          "default": "llama3.2",
          "markdownDescription": "Model name to use.\n\n**Examples:**\n- OpenAI: `gpt-4`, `gpt-3.5-turbo`, `gpt-4-turbo`\n- Ollama: `llama3.2`, `mistral`, `codellama`\n- Custom: `your-model-name`",
          "order": 3
        },
        "localLLM.temperature": {
          "type": "number",
          "default": 0.7,
          "minimum": 0.0,
          "maximum": 2.0,
          "markdownDescription": "Sampling temperature for model responses.\n- `0.0` = deterministic\n- `2.0` = very random",
          "order": 4
        },
        "localLLM.maxTokens": {
          "type": "number",
          "default": 2048,
          "minimum": 128,
          "maximum": 32768,
          "markdownDescription": "Maximum tokens for model responses.",
          "order": 5
        },
        "localLLM.systemPrompt": {
          "type": "string",
          "default": "You are a helpful coding assistant inside VS Code. Keep answers concise. When proposing file content, respond with a fenced code block beginning with ```file path=\"relative/path.ext\" followed by the complete file content.",
          "markdownDescription": "System prompt sent to the LLM to define its behavior.",
          "editPresentation": "multilineText",
          "order": 6
        },
        "localLLM.maxHistoryMessages": {
          "type": "number",
          "default": 50,
          "minimum": 5,
          "maximum": 200,
          "markdownDescription": "Maximum number of messages to keep in conversation history.",
          "order": 7
        },
        "localLLM.requestTimeout": {
          "type": "number",
          "default": 120000,
          "minimum": 10000,
          "maximum": 600000,
          "markdownDescription": "Request timeout in milliseconds (default: 120000 = 2 minutes).",
          "order": 8
        },
        "localLLM.maxFileSize": {
          "type": "number",
          "default": 1048576,
          "minimum": 1024,
          "maximum": 10485760,
          "markdownDescription": "Maximum file size in bytes for LLM-generated files (default: 1MB).",
          "order": 9
        },
        "localLLM.allowWriteWithoutPrompt": {
          "type": "boolean",
          "default": false,
          "markdownDescription": "If enabled, allow `/write` command to create/update files without confirmation.\n\n⚠️ **NOT recommended for security**",
          "order": 10
        }
      }
    }
  },
  "scripts": {
    "vscode:prepublish": "npm run compile",
    "compile": "tsc -p ./",
    "watch": "tsc -watch -p ./",
    "lint": "eslint src --ext ts"
  },
  "devDependencies": {
    "@types/node": "^20.11.0",
    "@types/vscode": "^1.85.0",
    "typescript": "^5.4.0"
  },
  "dependencies": {}
}
