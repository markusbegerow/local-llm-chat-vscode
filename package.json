{
  "name": "local-llm-chat-vscode",
  "displayName": "Local LLM for VS Code",
  "description": "Secure chat with local LLMs and create files in your workspace with AI assistance.",
  "version": "1.0.1",
  "publisher": "MarkusBegerow",
  "engines": {
    "vscode": "^1.85.0"
  },
  "categories": [
    "Other",
    "Machine Learning",
    "Snippets"
  ],
  "repository": {
    "type": "git",
    "url": "https://github.com/markusbegerow/local-llm-chat-vscode"
  },
  "homepage": "https://github.com/markusbegerow/local-llm-chat-vscode#readme",
  "bugs": { "url": "https://github.com/markusbegerow/local-llm-chat-vscode/issues" },
  "license": "SEE LICENSE IN LICENSE",
  "keywords": [
    "llm",
    "ollama",
    "ai",
    "chat",
    "code-assistant"
  ],
  "activationEvents": [
    "onCommand:localLLM.openChat",
    "onCommand:localLLM.setCredentials",
    "onCommand:localLLM.newFileFromSelection",
    "onCommand:localLLM.clearConversation",
    "onCommand:localLLM.sendFileToChat",
    "onCommand:localLLM.sendActiveFileToChat",
    "onCommand:localLLM.listWorkspaceFiles",
    "onCommand:localLLM.getWorkspaceInfo",
    "onCommand:localLLM.searchFiles"
  ],
  "main": "./out/extension.js",
  "contributes": {
    "commands": [
      {
        "command": "localLLM.openChat",
        "title": "Local LLM: Open Chat",
        "icon": "$(comment-discussion)"
      },
      {
        "command": "localLLM.setCredentials",
        "title": "Local LLM: Configure API Settings"
      },
      {
        "command": "localLLM.newFileFromSelection",
        "title": "Local LLM: Create File From Selection"
      },
      {
        "command": "localLLM.clearConversation",
        "title": "Local LLM: Clear Conversation History"
      },
      {
        "command": "localLLM.sendFileToChat",
        "title": "Local LLM: Send File to Chat"
      },
      {
        "command": "localLLM.sendActiveFileToChat",
        "title": "Local LLM: Send Active File to Chat"
      },
      {
        "command": "localLLM.listWorkspaceFiles",
        "title": "Local LLM: List Workspace Files"
      },
      {
        "command": "localLLM.getWorkspaceInfo",
        "title": "Local LLM: Get Workspace Info"
      },
      {
        "command": "localLLM.searchFiles",
        "title": "Local LLM: Search Files"
      }
    ],
    "configuration": {
      "title": "Local LLM Chat",
      "properties": {
        "localLLM.apiUrl": {
          "type": "string",
          "default": "http://localhost:11434",
          "description": "Base URL of your LLM API (e.g., http://localhost:11434 for Ollama, or http://localhost:1234 for LM Studio)."
        },
        "localLLM.model": {
          "type": "string",
          "default": "llama3.1",
          "description": "Model name to use for chat completions."
        },
        "localLLM.apiCompat": {
          "type": "string",
          "enum": [
            "openai",
            "ollama"
          ],
          "default": "openai",
          "enumDescriptions": [
            "OpenAI-compatible API (LM Studio, vLLM, etc.)",
            "Ollama native API"
          ],
          "description": "API compatibility mode. Choose 'openai' for OpenAI-compatible endpoints or 'ollama' for native Ollama API."
        },
        "localLLM.customEndpoint": {
          "type": "string",
          "default": "",
          "description": "Optional full endpoint URL (OpenAI-style). When set, requests are sent here directly (e.g., https://host/api/chat/completions)."
        },
        "localLLM.temperature": {
          "type": "number",
          "default": 0.7,
          "minimum": 0.0,
          "maximum": 2.0,
          "description": "Sampling temperature for model responses (0.0 = deterministic, 2.0 = very random)."
        },
        "localLLM.maxTokens": {
          "type": "number",
          "default": 2048,
          "minimum": 128,
          "maximum": 32768,
          "description": "Maximum tokens for model responses."
        },
        "localLLM.systemPrompt": {
          "type": "string",
          "default": "You are a helpful coding assistant inside VS Code. Keep answers concise. When proposing file content, respond with a fenced code block beginning with ```file path=\"relative/path.ext\" followed by the complete file content.",
          "description": "System prompt sent to the LLM to define its behavior."
        },
        "localLLM.maxHistoryMessages": {
          "type": "number",
          "default": 50,
          "minimum": 5,
          "maximum": 200,
          "description": "Maximum number of messages to keep in conversation history."
        },
        "localLLM.requestTimeout": {
          "type": "number",
          "default": 120000,
          "minimum": 10000,
          "maximum": 600000,
          "description": "Request timeout in milliseconds (default: 120000 = 2 minutes)."
        },
        "localLLM.maxFileSize": {
          "type": "number",
          "default": 1048576,
          "minimum": 1024,
          "maximum": 10485760,
          "description": "Maximum file size in bytes for LLM-generated files (default: 1MB)."
        },
        "localLLM.allowWriteWithoutPrompt": {
          "type": "boolean",
          "default": false,
          "description": "If true, allow /write command to create/update files without confirmation (NOT recommended for security)."
        }
      }
    }
  },
  "scripts": {
    "vscode:prepublish": "npm run compile",
    "compile": "tsc -p ./",
    "watch": "tsc -watch -p ./",
    "lint": "eslint src --ext ts"
  },
  "devDependencies": {
    "@types/node": "^20.11.0",
    "@types/vscode": "^1.85.0",
    "typescript": "^5.4.0"
  },
  "dependencies": {}
}
